<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Google Font (IBM Plex Sans Condensed) -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans+Condensed:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&display=swap"
        rel="stylesheet"
    >

    <!-- Bootstrap (5.2.3) -->
    <link rel="stylesheet" href="../../css/bootstrap.css">  <!-- %UPDATE% -->
    <script defer src="../../js/bootstrap.bundle.min.js"></script>  <!-- %UPDATE% -->

    <!-- Favicon. -->
    <link rel="apple-touch-icon" sizes="180x180" href="/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
    <link rel="manifest" href="/favicon/site.webmanifest">
    <link rel="mask-icon" href="/favicon/safari-pinned-tab.svg" color="#5bbad5">
    <link rel="shortcut icon" href="/favicon/favicon.ico">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="msapplication-config" content="/favicon/browserconfig.xml">
    <meta name="theme-color" content="#ffffff">

    <link rel="stylesheet" href="../../css/website-base.css">  <!-- %UPDATE% -->
    <title>ML Tutorials: Unsupervised Learning Algorithms</title>
</head>
<body>
    <!-- Header for title of webpage. -->
    <header class="container-fluid">
        <div class="row py-4 bg-primary">
            <h1 class="col text-center text-white">
                MACHINE LEARNING TUTORIALS
            </h1>
        </div>
    </header>
    <!--- Navigation banner. -->
    <nav class="navbar navbar-expand-lg sticky-top py-0 navbar-dark bg-secondary">
        <!-- Container for entire navbar. -->
        <div class="container-fluid justify-content-end px-1">
            <!-- Hamburger menu that appears when screen gets smaller. -->
            <button class="navbar-toggler my-1" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNavDropdown">
                <span class="navbar-toggler-icon"></span>
            </button>
            <!-- Container for navbar items. -->
            <div class="collapse navbar-collapse justify-content-end" id="navbarNavDropdown">
                <ul class="navbar-nav">
                    <li class="nav-item mx-4">
                        <a class="nav-link text-center" href="../../index.html">Home</a>  <!-- %UPDATE% -->
                    </li>
                    <li class="nav-item mx-4">
                        <a class="nav-link text-center" href="../../tutorials.html">Tutorials</a>  <!-- %UPDATE% -->
                    </li>
                </ul>
            </div>
        </div>
    </nav>
    <!-- Outer wrapper for main content. -->
    <div class="container-fluid" id="contentWrapper">
        <div class="row">
            <div class="col-md-1 col-lg-2 col-xxl-3"></div>
            <!-- Container for main content. -->
            <div class="col-12 col-md-10 col-lg-8 col-xxl-6 my-md-4 py-3 py-sm-4 px-0 px-sm-3 bg-white">
                <section class="container-fluid">
                    <!-- Title. -->
                    <div class="row">
                        <h2 class="col mb-3 mx-2 mx-sm-1 text-start border-start border-3 border-secondary text-primary">
                            ML Tutorials: Unsupervised Learning Algorithms
                        </h2>
                    </div>
                    <!-- Title/image. -->
                    <div class="row">
                        <div class="d-flex justify-content-center">
                            <img
                                class="img-fluid"
                                src="../../imgs/unsupervised-learning.webp"
                                alt="[Enter image here]"
                                width=100%
                            >  <!-- %UPDATE% -->
                        </div>
                    </div>
                    <!-- Description. -->
                    <div class="row mt-2 mb-3">
                        <div class="col">
                            <p class="pt-2">
                                Unsupervised Learning has been gaining momentum as Recommendation Systems have grown in popularity. These algorithms tend to be used in more advanced use cases, though understanding them is key to becoming an effective Data Scientist or ML Engineer. This tutorial will cover the following learning objectives:
                            </p>
                            <ul>
                                <li>K-Means Clustering</li>
                                <li>Principal Component Analysis (PCA)</li>
                                <li>Support Vector Machines (SVMs)</li>
                            </ul>
                        </div>
                    </div>
                    <div>
                        <h2 class="mt-5 px-4 px-sm-3 border-start border-3 border-secondary">K-Means Clustering</h2> <br>
                        <iframe width="650" height="400" src="https://www.youtube.com/embed/GZj6ikx8PAc?end=364" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe> <br><br>
                        <h4 class="pt-4">Summary</h4>
                        <ul>
                            <li class="pt-1"><b><i>K-Means</i></b> is the most common algorithm for conducting cluster analysis. "k" represents the number of clusters specified. This algorithm allows you to input a set of Training Data, specify a number of clusters, and use the algorithm to cluster the data points by the means of the features provided.</li>
                            <li class="pt-2">K-Means is considered "Unsupervised" because you are allowing the algorithm to assign the Training Data to specific classes rather than telling it which class each data point is assigned to.</li>
                            <li class="pt-2">To perform a cluster analysis with the K-Means alorith, use the following steps:
                                <ol class="pt-2">
                                    <li class="pt-1"><b>Define the number of clusters</b></li>
                                    <li class="pt-1"><b>Set cluster centers randomly</b>
                                    <ul>
                                        <li>The center for each cluster is known as a <b><i>Centroid</i></b></li>
                                    </ul></li>
                                    <li class="pt-1"><b>Assign points to clusters</b>
                                    <ul>
                                        <li>The algorithm will then measure the distance for each data point between each Centroid. The data point will be assigned to a cluster based on the closest Centroid.</li>
                                    </ul></li>
                                    <li><b>Calculate the center of each cluster</b>
                                    <ul>
                                        <li>Once the initial clusters have been created, the official Centroids will be established as close to the middle of the cluster as possible.</li>
                                    </ul></li>
                                    <li><b>Assign points to the new clusters</b>
                                    <ul>
                                        <li>With the new Centroids created, we'll now repeat step 3 by assigning each data point to a cluster based on the Centroid closest to it.</li>
                                    </ul></li>
                                    <li><b>Repeat Steps 4 and 5 Until Convergence is reached</b>
                                    <ul>
                                        <li><b><i>Convergence</i></b> is when every cluster is as equally distrributed as possible, given the centroid distances and locations.</li>
                                    </ul></li>
                                </ol>
                            </li>
                            <li class="pt-2">When using pre-built K-Means tools such as RapidMiner or SciKit-Learn, the initial clusters have been optimized to reduce biases.</li>
                            <li class="pt-2">The <b><i>Elbow Method</i></b> calculate the summed distance between the points and the Centroid where with every iteration, the another cluster is added to the algorithm until it meets the max number of clusters specified. In each interation, the summed distance between the points and the Centroid gets smaller. When anaylzing the results of the Elbow Method, you look for where the change in efficiency starts to plateau and that is considered the optimal number of clusters.</li>
                            <li class="pt-2"><b>NOTE:</b> K-Means is best used for creating classes for use in training a Decision Tree. For example, say you want to classify customers into three classes: Target Market, Common, and Less Valuable. You can use K-Means to cluster the customers based on their features and then use a Decision Tree Classifier and/or Random Forest to find which features best predict the classes</li>
                        </ul>
                        <h2 class="mt-5 px-4 px-sm-3 border-start border-3 border-secondary">Principal Component Analysis (PCA)</h2> <br>
                        <iframe width="650" height="400" src="https://www.youtube.com/embed/Ollp2nSQCLY?start=473" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe> <br><br>
                        <h4 class="pt-4">Summary</h4>
                        <ul>
                            <li class="pt-1"><b><i>Principal Component Analysis</i></b>, also known as <b><i>Exploratory Factor Analysis</i></b>, is a statistical method for identifying structures in your Training Data. This is useful when you have many features to consider and want to investigate the correlations between them. These correlations are the basis of PCA.</li>
                            <li class="pt-2">PCA is used to group features that are most highly correlated. This is used to create a sort of "cause and effect" diagram to visualize how one feature impacts another.</li>
                            <li class="pt-2">In PCA, a <b><i>factor</i></b> is a hidden variable that affects several other variables. For example, someone who smokes could have lung cancer. In this example, the factor is the smoking and the effect is the lung cancer.</li>
                            <li class="pt-2">Just like finding the optimal number of cluster in K-Means, the <b><i>Scree Test</i></b> allows you to create a descending line chart that shows the number of factors used in the iteration on the x-axis and the measure of effectiveness on the y-axis. However, when interpretting the "Screeplot", you choose the largest value that has an "eigenvalue" greater than 1 as the optimal number of factors to consider in the algorithm.</li>
                            <li class="pt-2"><b><i>Factor Loading</i></b> is the process of identifying how correlated each feature is with its assigned factor.</li>
                            <li class="pt-2">The <b><i>Eigenvalue</i></b> explains how effective the number of factors is at explaining the amount of variance across all variables. The reason why you don't want to choose the number of factors with the highest eigenvalue is because you want to find a perfect medium between an even distribution and a highly explained variance.</li>
                            <li class="pt-2"><b>NOTE:</b> When developing a PCA in a pre-built tool such as RapidMiner or SciKit-Learn, the rotated component matrix will automatically be applied to allow you to best analyze the results without any bias.</li>
                        </ul>
                        <h2 class="mt-5 px-4 px-sm-3 border-start border-3 border-secondary">Support Vector Machines</h2> <br>
                        <iframe width="650" height="400" src="https://www.youtube.com/embed/CPqOCI0ahss?" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe> <br><br>
                        <h4 class="pt-4">Summary</h4>
                        <ul>
                            <li class="pt-1"><b><i>Naive Bayes</i></b> is a common classification algorithm used by Data Scientists to determine the class of a target variable based on categorical independent variables. This algorithm can be used for either Binary Classification or Multiclass Classification.</li>
                            <li class="pt-2">The primary difference between Naive Bayes and Logistic Regression is that <b><i>Logistic Regression</i></b> requires numeric independent variables, whereas <b><i>Naive Bayes</i></b> can work with string-based, or categorical, independent variables.</li>
                            <li class="pt-2">Naive Bayes is based on Bayes Theorem, a conditional theorem, which can be described as an evidence-based trust theorem (the more evidence present to support the condition, the more trust is given). Bayes Theorem is based on the concept of "how much you should trust the evidence". Bayes Theorem is represented by the following equation: <br>
                                <b>P(A|C) = P(C|A) * P(A)/ P(C)</b>
                            <ul>
                                <li class="pt-1"><b>P(A)</b> equals the <b><i>Prior Probability</i></b>, which describes the degree to which we believe the model accurately describes reality based on all of our prior information.</li>
                                <li class="pt-1"><b>P(C|A)</b> equals the <b><i>Likelihood</i></b>, which describes how well the model predicts the data.</li>
                                <li class="pt-1"><b>P(C)</b> equals the <b><i>Normalizing Constant</i></b>, which is the constant that makes the Posterior Density integrate to one (add up all probabilities to 1.0).</li>
                                <li class="pt-1"><b>P(A|C)</b> equals the <b><i>Posterior Probability</i></b>, which represents the degree to which we believe a given model accurately describes the situation given the Training Data.</li>
                            </ul></li>
                            <li class="pt-2">The reason why Naive Bayes is "Naive" is because if there are two or more features that are not correlated, then the probability of seeing those features in the same class is just the product of all the probabilities.</li>
                            <li class="pt-2"><b>Naive Bayes Pros and Cons:</b>
                                <ul>
                                    <li><b>Pros:</b>
                                    <ul>
                                        <li>It's fast and easy to predict a class.</li>
                                        <li>It provides better performance than Logistic Regression when conducting Binary Classification.</li>
                                        <li>It performs well in the case of categorical features compared to numerical features.</li>
                                    </ul></li>
                                    <li><b>Cons:</b>
                                    <ul>
                                        <li>If you forget to include all possible classes of the target variable in the Training Data, the Tetsing Data will predict a 0 probability score for the samples matching that missing class. Although this can be fixed using smoothing techniques, it's just another step to work through.</li>
                                        <li>Since Naive Bayes heavily relies on probabilities, it's bad at estimating classes based on feature correlations.</li>
                                        <li>If independent features are present, which can be defined as features that are not correlated to any other features, this can cause bias in the classification process.</li>
                                    </ul></li>
                                </ul></li>
                            <li class="pt-2"><b>NOTE:</b> Naive Bayes is traditionally only used when real-time decisions need to be made, since Decision Trees, Random Forests, and Neural Networks tend to provide much better results with lower error rates.</li>
                        </ul>
                        <h2 class="mt-5 px-4 px-sm-3 border-start border-3 border-secondary">Decision Trees</h2> <br>
                        <h6><b>Decision Tree Classification</b></h6>
                        <iframe width="650" height="400" src="https://www.youtube.com/embed/ZVR2Way4nwQ?" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe> <br><br>
                        <h6><b>Decision Tree Regression</b></h6>
                        <iframe width="650" height="400" src="https://www.youtube.com/embed/UhY5vPfQIrA?" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe> <br><br>
                        <h4 class="pt-4">Summary</h4>
                        <ul>
                            <li class="pt-1"><b><i>Decision Trees</i></b> are binary trees that recursively split the Training Data into separate classes until a prediction is returned. These are commonly used by Data Scientists and ML Engineers to conduct Binary and Multi-Class Classification.</li>
                            <li class="pt-2">There are three types of nodes within a Decision Tree: The <b><i>Root Node</i></b> is the base of the tree and create the first split of the data based on a specific condition, <b><i>Decision Nodes</i></b> further split the data until it has reached a specific threshold, and <b><i>Leaf Nodes</i></b> contain the final predictions that assign a class to the data points inside.</li>
                            <li class="pt-2"><b><i>Pure Leaf Nodes</i></b> occur when all the data points in a Leaf Node are assigned to a single class. When you have a large number of data points, this may not be possible.</li>
                            <li class="pt-2"><b><i>Information Gain</i></b> is the process used by Decision Trees to find the optimal conditions to create pure leaf nodes. The best possible information gain split is where the data gets split evenly into two classes. When working with more than two classes, the best possible scenario is to find the smallest amount of conditions to create pure leaf nodes.</li>
                            <li class="pt-2"><b><i>Entropy</i></b> is the measure of information contained in a state. In the context of Decision Trees, the state would be the subset of data contained in each Decision Node. The higher the Entropy, the less amount of clear information is present in the Decision Node.</li>
                            <li class="pt-2">When using a Decision Tree for Regression, also known as a <b><i>Regression Tree</i></b>, you'll run into leaf nodes that have more than one data point. Rather than assigning each data point the same value, we find the mean of the data points by summing them and dividing by the number of points in the leaf node. This will produce the prediction for the target variable.</li>
                            <li class="pt-2"><b><i>Variance Reduction</i></b> is a Regression technique in which the goal is to reduce the amount of space between the data points. When visualizing a Regression model, you'll view the "line of best fit", the <b><i>line of best fit</i></b>, or <b><i>trend line</i></b>, represents all the predicted values. This is created by finding either the Root Mean Squared Error (RMSE) or Sqaured Correlation (r-squared). We'll discuss these metrics in a later tutorial.</li>
                            <li class="pt-2">In the context of Regression Trees, <b><i>Variance Reduction</i></b> is used to reduce the amount of space between the data points in each decision node. This is the equivalent of how Entropy is used to determine the ideal split condition for each Decision Node.</li>
                            <li class="pt-2"><b>NOTE:</b> Decision Trees are the best Supervised Learning algorithm for both Classification and Regression when working with categorical features (e.g., make and model of a car).</li>
                        </ul>
                        <h2 class="mt-5 px-4 px-sm-3 border-start border-3 border-secondary">Random Forests</h2> <br>
                        <iframe width="650" height="400" src="https://www.youtube.com/embed/v6VJ2RO66Ag?start=473" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe> <br><br>
                        <h4 class="pt-4">Summary</h4>
                        <ul>
                            <li class="pt-1"><b><i>Random Forests</i></b> are a collection of Decision Trees that are used by Data Scientists and ML Engineers to reduce Overfitting (a concept we will discuss in a later tutorial).</li>
                            <li class="pt-2">When creating a Random Forest, you must specify the number of trees you'd like to fit your Training Data to. <b><i>Bootstrapping</i></b> is the concept of grabbing random samples and features from the training dataset and creating subsets to be used in each tree. Each subset will contain the same number of rows as the original Training Data, but NOT the same number of features.</li>
                            <li class="pt-2">When using a Random Forest for Classification, new data points are passed through each Decision Tree and the class with the most "votes" from each tree gets assigned. This helps your model understand the importance of each individual feature and how they interact with each other in the context of correlation.</li>
                            <li class="pt-2"><b><i>Aggregation</i></b> is the process of collecting the results from each Decision Tree created by the Random Forest and counting the number of occurrences of each class from all trees. The class with the highest number of occurrences gets assigned to the data point.</li>
                            <li class="pt-2">Random Forests are better than basic Decision Trees becuase they allow your model to give weights to features based on the impact they have on predictions. If a single feature consistently provides the same predictions as other trees, it's considered to be very important.</li>
                            <li class="pt-2">When deciding how many features to include in each Decision Tree, it's wise to use the log or square root of the number of features available. This has been found to be the best method to reduce observational bias.</li>
                            <li class="pt-2">When used with Regression problems, rather than aggregating the count of each class, the mean of all predictions is taken to provide the output prediciton for each data point.</li>
                            <li class="pt-2"><b>NOTE:</b> Whenever you suspect bias in your data, or any form of uneven distribution, always go with Random Forests. This will reduce overfitting and focus more on important features rather than equal weights on all features.</li>
                        </ul><br>
                        <!-- Previous/Next Topic buttons. -->
                    <div class="row mt-4 mt-md-5 mb-2 px-3">
                        <div class="col-0 col-sm-8">
                            <a
                            class="col-12 col-sm-4 btn btn-outline-primary py-2 fs-5"
                            href="../Machine Learning/Supervised-Algorithms.html">
                            Previous Topic
                            </a>
                        </div>
                        <a
                            class="col-12 col-sm-4 btn btn-outline-primary py-2 fs-5"
                            href="../Machine Learning/Unsupervised-Algorithms.html">
                            Next Topic
                        </a>
                    </div>
                </section>
            </div>
            <div class="col-md-1 col-lg-2 col-xxl-3"></div>
        </div>
    </div>
    <!-- Footer at bottom of page. -->
    <footer class="container-fluid bg-secondary">
        <div class="row align-items-center h-100">
            <h4 class="col m-0 text-center text-white">
                © 2023 Data Science Club @ UVU
            </h4>
        </div>
    </footer>
</body>
</html>
