<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Google Font (IBM Plex Sans Condensed) -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans+Condensed:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&display=swap"
        rel="stylesheet"
    >

    <!-- Bootstrap (5.2.3) -->
    <link rel="stylesheet" href="../../css/bootstrap.css">  <!-- %UPDATE% -->
    <script defer src="../../js/bootstrap.bundle.min.js"></script>  <!-- %UPDATE% -->

    <!-- Favicon. -->
    <link rel="apple-touch-icon" sizes="180x180" href="/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
    <link rel="manifest" href="/favicon/site.webmanifest">
    <link rel="mask-icon" href="/favicon/safari-pinned-tab.svg" color="#5bbad5">
    <link rel="shortcut icon" href="/favicon/favicon.ico">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="msapplication-config" content="/favicon/browserconfig.xml">
    <meta name="theme-color" content="#ffffff">

    <link rel="stylesheet" href="../../css/website-base.css">  <!-- %UPDATE% -->
    <title>ML Tutorials: Reinforcement Learning Algorithms</title>
</head>
<body>
    <!-- Header for title of webpage. -->
    <header class="container-fluid">
        <div class="row py-4 bg-primary">
            <h1 class="col text-center text-white">
                MACHINE LEARNING TUTORIALS
            </h1>
        </div>
    </header>
    <!--- Navigation banner. -->
    <nav class="navbar navbar-expand-lg sticky-top py-0 navbar-dark bg-secondary">
        <!-- Container for entire navbar. -->
        <div class="container-fluid justify-content-end px-1">
            <!-- Hamburger menu that appears when screen gets smaller. -->
            <button class="navbar-toggler my-1" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNavDropdown">
                <span class="navbar-toggler-icon"></span>
            </button>
            <!-- Container for navbar items. -->
            <div class="collapse navbar-collapse justify-content-end" id="navbarNavDropdown">
                <ul class="navbar-nav">
                    <li class="nav-item mx-4">
                        <a class="nav-link text-center" href="../../index.html">Home</a>  <!-- %UPDATE% -->
                    </li>
                    <li class="nav-item mx-4">
                        <a class="nav-link text-center" href="../../tutorials.html">Tutorials</a>  <!-- %UPDATE% -->
                    </li>
                </ul>
            </div>
        </div>
    </nav>
    <!-- Outer wrapper for main content. -->
    <div class="container-fluid" id="contentWrapper">
        <div class="row">
            <div class="col-md-1 col-lg-2 col-xxl-3"></div>
            <!-- Container for main content. -->
            <div class="col-12 col-md-10 col-lg-8 col-xxl-6 my-md-4 py-3 py-sm-4 px-0 px-sm-3 bg-white">
                <section class="container-fluid">
                    <!-- Title. -->
                    <div class="row">
                        <h2 class="col mb-3 mx-2 mx-sm-1 text-start border-start border-3 border-secondary text-primary">
                            ML Tutorials: Reinforcement Learning Algorithms
                        </h2>
                    </div>
                    <!-- Title/image. -->
                    <div class="row">
                        <div class="d-flex justify-content-center">
                            <img
                                class="img-fluid"
                                src="../../imgs/reinforcement-learning.webp"
                                alt="[Enter image here]"
                                width=100%
                            >  <!-- %UPDATE% -->
                        </div>
                    </div>
                    <!-- Description. -->
                    <div class="row mt-2 mb-3">
                        <div class="col">
                            <p class="pt-2">
                                Reinforcement Learning is a newer branch of Data Science focused on allowing the machine to learn from itself, just as a dog would when learning how to respond to basic commands. Since this is a more advanced topic, this tutotial will be VERY high level. This tutorial will cover the following learning objectives:
                            </p>
                            <ul>
                                <li>Reinforcement Learning Overview</li>
                                <li>Model-based RL vs. Model-free RL</li>
                                <li>Principal Component Analysis (PCA)</li>

                            </ul>
                        </div>
                    </div>
                    <div>
                        <h2 class="mt-5 px-4 px-sm-3 border-start border-3 border-secondary">Reinforcement Learning Overview</h2> <br>
                        <iframe width="650" height="400" src="https://www.youtube.com/embed/i7q8bISGwMQ?start=157&end=435" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe> <br><br>
                        <h4 class="pt-4">Summary</h4>
                        <ul>
                            <li class="pt-1">In <b><i>Reinforcement Learning</i></b>, the agent interacts with an environment and is controlled by a policy that states what it can and can't do. The agent performs actions based on a reward. The user (you) changes the policy based on the agent's state.</li>
                            <li class="pt-2">A common real-world example of Reinforcement Learning is training a dog to sit. Typically, you'll first give the command (tell the dog "Sit!") and get its initial reaction to set a baseline. You then use a reward system, such as a treat or toy, to reinforce good behavior and prevent bad behavior. In this example, the <b><i>agent</i></b> is the dog, the <b><i>environment</i></b> is your home, or wherever you're training the dog, the <b><i>reward</i></b> is the treat or toy, the <b><i>policy</i></b> is the command you're giving, the <b><i>action</i></b> is the dog's response to the policy, and the <b><i>state</i></b> is the dog's response to the policy in the environment. However, the state can change based on the current, state. For example, if the dog is already sitting, what will it do when you tell it to sit?
                            </li>
                            <li class="pt-2">A <b><i>Value Function</i></b> is a probability measure that states, given the current environment and present states, what is the probability of the policy returning a positive action? The <b><i>Discount Rate</i></b> is a variable in the function that determines the likelihood of the action occurring based on previous situations in the same environment. If the Discount Rate is high, that probably means the action has become a habit for the machine (or for the dog in the example above).</li>
                            <li class="pt-2">The overall goal of Reinforcement Learning is to optimize the policy to maximize future rewards. Following the example above, you could adjust the policy of telling the dog "Sit!" by changing your tone, volume, or facial expressions to identify how the agent (the dog) best responds.</li>
                        </ul>
                        <h2 class="mt-5 px-4 px-sm-3 border-start border-3 border-secondary">Model-based RL vs. Model-free RL</h2> <br>
                        <iframe width="650" height="400" src="https://www.youtube.com/embed/i7q8bISGwMQ?start=473" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe> <br><br>
                        <h4 class="pt-4">Summary</h4>
                        <ul>
                            <li class="pt-1"><b><i>Model-based RL</i></b> follows a decision-making process, such as the <b><i>Markov Decision Process</i></b>, as a framework for creating and adjusting a policy. These are commonly used when the problem you're trying to solve is based on probability rather than random state.</li>
                            <li class="pt-2"><b><i>Model-based RL</i></b> sets a pre-configured starting state and then follows the process of <b><i>Policy Iteration</i></b> to then iteratively compare each combination of variable to get the best possible outcome. Within Policy Iteration, the process of <b><i>Value Iteration</i></b> is used to compute the outcome of each policy combination to then create the reward based on the efficiency and effectiveness of the policy.</li>
                            <li class="pt-2"><b><i>Model-based RL</i></b> can also be used with non-linear issues, such as self-driving cars which can take potentially thousands of possible actions to achieve the desired outcome. </li>
                            <li class="pt-2"><b><i>Model-free RL</i></b> can be broken out into two sub-categories: Gradient free and Graident based.</li>
                            <li class="pt-2"><b>NOTE:</b> Hierarchical Clustering is very computationally expensive, meaning that the more data points you have, and the more potential clusters that can be created, the computing power necessary increases exponentially (starting at a certain point, differs between systems).</li>
                        </ul>
                        <h2 class="mt-5 px-4 px-sm-3 border-start border-3 border-secondary">Principal Component Analysis (PCA)</h2> <br>
                        <iframe width="650" height="400" src="https://www.youtube.com/embed/Ollp2nSQCLY?start=473" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe> <br><br>
                        <h4 class="pt-4">Summary</h4>
                        <ul>
                            <li class="pt-1"><b><i>Principal Component Analysis</i></b>, also known as <b><i>Exploratory Factor Analysis</i></b>, is a statistical method for identifying structures in your Training Data. This is useful when you have many features to consider and want to investigate the correlations between them. These correlations are the basis of PCA.</li>
                            <li class="pt-2">PCA is used to group features that are most highly correlated. This is used to create a sort of "cause and effect" diagram to visualize how one feature impacts another.</li>
                            <li class="pt-2">In PCA, a <b><i>factor</i></b> is a hidden variable that affects several other variables. For example, someone who smokes could have lung cancer. In this example, the factor is the smoking and the effect is the lung cancer.</li>
                            <li class="pt-2">Just like finding the optimal number of cluster in K-Means, the <b><i>Scree Test</i></b> allows you to create a descending line chart that shows the number of factors used in the iteration on the x-axis and the measure of effectiveness on the y-axis. However, when interpretting the "Screeplot", you choose the largest value that has an "eigenvalue" greater than 1 as the optimal number of factors to consider in the algorithm.</li>
                            <li class="pt-2"><b><i>Factor Loading</i></b> is the process of identifying how correlated each feature is with its assigned factor.</li>
                            <li class="pt-2">The <b><i>Eigenvalue</i></b> explains how effective the number of factors is at explaining the amount of variance across all variables. The reason why you don't want to choose the number of factors with the highest eigenvalue is because you want to find a perfect medium between an even distribution and a highly explained variance.</li>
                            <li class="pt-2"><b>NOTE:</b> When developing a PCA in a pre-built tool such as RapidMiner or SciKit-Learn, the rotated component matrix will automatically be applied to allow you to best analyze the results without any bias.</li>
                        </ul><br>
                        <!-- Previous/Next Topic buttons. -->
                    <div class="row mt-4 mt-md-5 mb-2 px-3">
                        <div class="col-0 col-sm-8">
                            <a
                            class="col-12 col-sm-4 btn btn-outline-primary py-2 fs-5"
                            href="../Machine Learning/Unsupervised-Algorithms.html">
                            Previous Topic
                            </a>
                        </div>
                        <a
                            class="col-12 col-sm-4 btn btn-outline-primary py-2 fs-5"
                            href="../Machine Learning/Reinforcement-Algorithms.html">
                            Next Topic
                        </a>
                    </div>
                </section>
            </div>
            <div class="col-md-1 col-lg-2 col-xxl-3"></div>
        </div>
    </div>
    <!-- Footer at bottom of page. -->
    <footer class="container-fluid bg-secondary">
        <div class="row align-items-center h-100">
            <h4 class="col m-0 text-center text-white">
                © 2023 Data Science Club @ UVU
            </h4>
        </div>
    </footer>
</body>
</html>
