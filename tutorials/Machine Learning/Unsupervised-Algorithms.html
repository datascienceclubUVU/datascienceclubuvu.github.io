<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Google Font (IBM Plex Sans Condensed) -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans+Condensed:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&display=swap"
        rel="stylesheet"
    >

    <!-- Bootstrap (5.2.3) -->
    <link rel="stylesheet" href="../../css/bootstrap.css">  <!-- %UPDATE% -->
    <script defer src="../../js/bootstrap.bundle.min.js"></script>  <!-- %UPDATE% -->

    <!-- Favicon. -->
    <link rel="apple-touch-icon" sizes="180x180" href="/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
    <link rel="manifest" href="/favicon/site.webmanifest">
    <link rel="mask-icon" href="/favicon/safari-pinned-tab.svg" color="#5bbad5">
    <link rel="shortcut icon" href="/favicon/favicon.ico">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="msapplication-config" content="/favicon/browserconfig.xml">
    <meta name="theme-color" content="#ffffff">

    <link rel="stylesheet" href="../../css/website-base.css">  <!-- %UPDATE% -->
    <title>ML Tutorials: Unsupervised Learning Algorithms</title>
</head>
<body>
    <!-- Header for title of webpage. -->
    <header class="container-fluid">
        <div class="row py-4 bg-primary">
            <h1 class="col text-center text-white">
                MACHINE LEARNING TUTORIALS
            </h1>
        </div>
    </header>
    <!--- Navigation banner. -->
    <nav class="navbar navbar-expand-lg sticky-top py-0 navbar-dark bg-secondary">
        <!-- Container for entire navbar. -->
        <div class="container-fluid justify-content-end px-1">
            <!-- Hamburger menu that appears when screen gets smaller. -->
            <button class="navbar-toggler my-1" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNavDropdown">
                <span class="navbar-toggler-icon"></span>
            </button>
            <!-- Container for navbar items. -->
            <div class="collapse navbar-collapse justify-content-end" id="navbarNavDropdown">
                <ul class="navbar-nav">
                    <li class="nav-item mx-4">
                        <a class="nav-link text-center" href="../../index.html">Home</a>  <!-- %UPDATE% -->
                    </li>
                    <li class="nav-item mx-4">
                        <a class="nav-link text-center" href="../../tutorials.html">Tutorials</a>  <!-- %UPDATE% -->
                    </li>
                </ul>
            </div>
        </div>
    </nav>
    <!-- Outer wrapper for main content. -->
    <div class="container-fluid" id="contentWrapper">
        <div class="row">
            <div class="col-md-1 col-lg-2 col-xxl-3"></div>
            <!-- Container for main content. -->
            <div class="col-12 col-md-10 col-lg-8 col-xxl-6 my-md-4 py-3 py-sm-4 px-0 px-sm-3 bg-white">
                <section class="container-fluid">
                    <!-- Title. -->
                    <div class="row">
                        <h2 class="col mb-3 mx-2 mx-sm-1 text-start border-start border-3 border-secondary text-primary">
                            ML Tutorials: Unsupervised Learning Algorithms
                        </h2>
                    </div>
                    <!-- Title/image. -->
                    <div class="row">
                        <div class="d-flex justify-content-center">
                            <img
                                class="img-fluid"
                                src="../../imgs/unsupervised-learning.webp"
                                alt="[Enter image here]"
                                width=100%
                            >  <!-- %UPDATE% -->
                        </div>
                    </div>
                    <!-- Description. -->
                    <div class="row mt-2 mb-3">
                        <div class="col">
                            <p class="pt-2">
                                Unsupervised Learning has been gaining momentum as Recommendation Systems have grown in popularity. These algorithms tend to be used in more advanced use cases, though understanding them is key to becoming an effective Data Scientist or ML Engineer. This tutorial will cover the following learning objectives:
                            </p>
                            <ul>
                                <li>K-Means Clustering</li>
                                <li>Hierarchical Clustering</li>
                                <li>Principal Component Analysis (PCA)</li>

                            </ul>
                        </div>
                    </div>
                    <div>
                        <h2 class="mt-5 px-4 px-sm-3 border-start border-3 border-secondary">K-Means Clustering</h2> <br>
                        <iframe width="650" height="400" src="https://www.youtube.com/embed/GZj6ikx8PAc?end=364" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe> <br><br>
                        <h4 class="pt-4">Summary</h4>
                        <ul>
                            <li class="pt-1"><b><i>K-Means</i></b> is the most common algorithm for conducting cluster analysis. "k" represents the number of clusters specified. This algorithm allows you to input a set of Training Data, specify a number of clusters, and use the algorithm to cluster the data points by the means of the features provided.</li>
                            <li class="pt-2">K-Means is considered "Unsupervised" because you are allowing the algorithm to assign the Training Data to specific classes rather than telling it which class each data point is assigned to.</li>
                            <li class="pt-2">To perform a cluster analysis with the K-Means alorith, use the following steps:
                                <ol class="pt-2">
                                    <li class="pt-1"><b>Define the number of clusters</b></li>
                                    <li class="pt-1"><b>Set cluster centers randomly</b>
                                    <ul>
                                        <li>The center for each cluster is known as a <b><i>Centroid</i></b></li>
                                    </ul></li>
                                    <li class="pt-1"><b>Assign points to clusters</b>
                                    <ul>
                                        <li>The algorithm will then measure the distance for each data point between each Centroid. The data point will be assigned to a cluster based on the closest Centroid.</li>
                                    </ul></li>
                                    <li><b>Calculate the center of each cluster</b>
                                    <ul>
                                        <li>Once the initial clusters have been created, the official Centroids will be established as close to the middle of the cluster as possible.</li>
                                    </ul></li>
                                    <li><b>Assign points to the new clusters</b>
                                    <ul>
                                        <li>With the new Centroids created, we'll now repeat step 3 by assigning each data point to a cluster based on the Centroid closest to it.</li>
                                    </ul></li>
                                    <li><b>Repeat Steps 4 and 5 Until Convergence is reached</b>
                                    <ul>
                                        <li><b><i>Convergence</i></b> is when every cluster is as equally distrributed as possible, given the centroid distances and locations.</li>
                                    </ul></li>
                                </ol>
                            </li>
                            <li class="pt-2">When using pre-built K-Means tools such as RapidMiner or SciKit-Learn, the initial clusters have been optimized to reduce biases.</li>
                            <li class="pt-2">The <b><i>Elbow Method</i></b> calculate the summed distance between the points and the Centroid where with every iteration, the another cluster is added to the algorithm until it meets the max number of clusters specified. In each interation, the summed distance between the points and the Centroid gets smaller. When anaylzing the results of the Elbow Method, you look for where the change in efficiency starts to plateau and that is considered the optimal number of clusters.</li>
                            <li class="pt-2"><b>NOTE:</b> K-Means is best used for creating classes for use in training a Decision Tree. For example, say you want to classify customers into three classes: Target Market, Common, and Less Valuable. You can use K-Means to cluster the customers based on their features and then use a Decision Tree Classifier and/or Random Forest to find which features best predict the classes</li>
                        </ul>
                        <h2 class="mt-5 px-4 px-sm-3 border-start border-3 border-secondary">Hierarchical Clustering</h2> <br>
                        <iframe width="650" height="400" src="https://www.youtube.com/embed/ijUMKMC4f9I?start=473" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe> <br><br>
                        <h4 class="pt-4">Summary</h4>
                        <ul>
                            <li class="pt-1"><b><i>Hierarchical Clustering</i></b> is a technique used for clustering clusters of groups into tiers. These tiers become more specific as you go down the list.</li>
                            <li class="pt-2">There are two types of Hierarchical Clustering: Agglomerative and Divisive. <b><i>Divisive Clustering</i></b> takes a top-down approach where all data points begin in the same cluster and then get broken down into sub-clusters based on similar features. <b><i>Agglomerative Clustering</i></b> takes a bottom-up approach where you start with individual data points and start grouping them into ever increasing groups based on similar features.</li>
                            <li class="pt-2"><b><i>Agglomerative Clustering</i></b> starts with each data point representing its own cluster, then using <b><i>euclidean distance</i></b>, the formula that is used in K-Means to find the distance between a data point and a Centroid, a new cluster gets created with two data points. This process gets repeated until one massive cluster remains.</li>
                            <li class="pt-2">A <b><i>Dendrogram</i></b> is the visualization of Hierarchical Clusters. This allows you to map how each data point relates to others and what features separate each cluster. The distance between links in the Dendrogram show how similar or different two data points are. The closer they are, the more closely they are related.</li>
                            <li class="pt-2"><b>NOTE:</b> Hierarchical Clustering is very computationally expensive, meaning that the more data points you have, and the more potential clusters that can be created, the computing power necessary increases exponentially (starting at a certain point, differs between systems).</li>
                        </ul>
                        <h2 class="mt-5 px-4 px-sm-3 border-start border-3 border-secondary">Principal Component Analysis (PCA)</h2> <br>
                        <iframe width="650" height="400" src="https://www.youtube.com/embed/Ollp2nSQCLY?start=473" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe> <br><br>
                        <h4 class="pt-4">Summary</h4>
                        <ul>
                            <li class="pt-1"><b><i>Principal Component Analysis</i></b>, also known as <b><i>Exploratory Factor Analysis</i></b>, is a statistical method for identifying structures in your Training Data. This is useful when you have many features to consider and want to investigate the correlations between them. These correlations are the basis of PCA.</li>
                            <li class="pt-2">PCA is used to group features that are most highly correlated. This is used to create a sort of "cause and effect" diagram to visualize how one feature impacts another.</li>
                            <li class="pt-2">In PCA, a <b><i>factor</i></b> is a hidden variable that affects several other variables. For example, someone who smokes could have lung cancer. In this example, the factor is the smoking and the effect is the lung cancer.</li>
                            <li class="pt-2">Just like finding the optimal number of cluster in K-Means, the <b><i>Scree Test</i></b> allows you to create a descending line chart that shows the number of factors used in the iteration on the x-axis and the measure of effectiveness on the y-axis. However, when interpretting the "Screeplot", you choose the largest value that has an "eigenvalue" greater than 1 as the optimal number of factors to consider in the algorithm.</li>
                            <li class="pt-2"><b><i>Factor Loading</i></b> is the process of identifying how correlated each feature is with its assigned factor.</li>
                            <li class="pt-2">The <b><i>Eigenvalue</i></b> explains how effective the number of factors is at explaining the amount of variance across all variables. The reason why you don't want to choose the number of factors with the highest eigenvalue is because you want to find a perfect medium between an even distribution and a highly explained variance.</li>
                            <li class="pt-2"><b>NOTE:</b> When developing a PCA in a pre-built tool such as RapidMiner or SciKit-Learn, the rotated component matrix will automatically be applied to allow you to best analyze the results without any bias.</li>
                        </ul><br>
                        <!-- Previous/Next Topic buttons. -->
                    <div class="row mt-4 mt-md-5 mb-2 px-3">
                        <div class="col-0 col-sm-8">
                            <a
                            class="col-12 col-sm-4 btn btn-outline-primary py-2 fs-5"
                            href="../Machine Learning/Supervised-Algorithms.html">
                            Previous Topic
                            </a>
                        </div>
                        <a
                            class="col-12 col-sm-4 btn btn-outline-primary py-2 fs-5"
                            href="../Machine Learning/Unsupervised-Algorithms.html">
                            Next Topic
                        </a>
                    </div>
                </section>
            </div>
            <div class="col-md-1 col-lg-2 col-xxl-3"></div>
        </div>
    </div>
    <!-- Footer at bottom of page. -->
    <footer class="container-fluid bg-secondary">
        <div class="row align-items-center h-100">
            <h4 class="col m-0 text-center text-white">
                © 2023 Data Science Club @ UVU
            </h4>
        </div>
    </footer>
</body>
</html>
