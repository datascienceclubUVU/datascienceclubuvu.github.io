<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Google Font (IBM Plex Sans Condensed) -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans+Condensed:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&display=swap"
        rel="stylesheet"
    >

    <!-- Bootstrap (5.2.3) -->
    <link rel="stylesheet" href="../../css/bootstrap.css">  <!-- %UPDATE% -->
    <script defer src="../../js/bootstrap.bundle.min.js"></script>  <!-- %UPDATE% -->

    <!-- Favicon. -->
    <link rel="apple-touch-icon" sizes="180x180" href="/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
    <link rel="manifest" href="/favicon/site.webmanifest">
    <link rel="mask-icon" href="/favicon/safari-pinned-tab.svg" color="#5bbad5">
    <link rel="shortcut icon" href="/favicon/favicon.ico">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="msapplication-config" content="/favicon/browserconfig.xml">
    <meta name="theme-color" content="#ffffff">

    <link rel="stylesheet" href="../../css/website-base.css">  <!-- %UPDATE% -->
    <title>ML Tutorials: Feature Engineering</title>
</head>
<body>
    <!-- Header for title of webpage. -->
    <header class="container-fluid">
        <div class="row py-4 bg-primary">
            <h1 class="col text-center text-white">
                MACHINE LEARNING TUTORIALS
            </h1>
        </div>
    </header>
    <!--- Navigation banner. -->
    <nav class="navbar navbar-expand-lg sticky-top py-0 navbar-dark bg-secondary">
        <!-- Container for entire navbar. -->
        <div class="container-fluid justify-content-end px-1">
            <!-- Hamburger menu that appears when screen gets smaller. -->
            <button class="navbar-toggler my-1" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNavDropdown">
                <span class="navbar-toggler-icon"></span>
            </button>
            <!-- Container for navbar items. -->
            <div class="collapse navbar-collapse justify-content-end" id="navbarNavDropdown">
                <ul class="navbar-nav">
                    <li class="nav-item mx-4">
                        <a class="nav-link text-center" href="../../index.html">Home</a>  <!-- %UPDATE% -->
                    </li>
                    <li class="nav-item mx-4">
                        <a class="nav-link text-center" href="../../tutorials.html">Tutorials</a>  <!-- %UPDATE% -->
                    </li>
                </ul>
            </div>
        </div>
    </nav>
    <!-- Outer wrapper for main content. -->
    <div class="container-fluid" id="contentWrapper">
        <div class="row">
            <div class="col-md-1 col-lg-2 col-xxl-3"></div>
            <!-- Container for main content. -->
            <div class="col-12 col-md-10 col-lg-8 col-xxl-6 my-md-4 py-3 py-sm-4 px-0 px-sm-3 bg-white">
                <section class="container-fluid">
                    <!-- Title. -->
                    <div class="row">
                        <h2 class="col mb-3 mx-2 mx-sm-1 text-start border-start border-3 border-secondary text-primary">
                            ML Tutorials: Feature Engineering
                        </h2>
                    </div>
                    <!-- Title/image. -->
                    <div class="row">
                        <div class="d-flex justify-content-center">
                            <img
                                class="img-fluid"
                                src="../../imgs/feature-engineering.jpg"
                                alt="[Enter image here]"
                                width=100%
                            >  <!-- %UPDATE% -->
                        </div>
                    </div>
                    <!-- Description. -->
                    <div class="row mt-2 mb-3">
                        <div class="col">
                            <p class="pt-2">
                                Data is the fuel that keeps your models relevant and accurate. However, not all of it is helpful or useful. This is where Feature Engineering comes in. <b><i>Feature Engineering</i></b> ensures that the features you include in your model make an impact and are statistically significant. This tutorial will cover the following learning objectives:
                            </p>
                            <ul>
                                <li>What are Features?</li>
                                <li>What is Feature Engineering?</li>
                                <li>Feature Engineering Best Practices</li>

                            </ul>
                        </div>
                    </div>
                    <div>
                        <h2 class="mt-5 px-4 px-sm-3 border-start border-3 border-secondary">What are Features?</h2> <br>
                        <iframe width="650" height="400" src="https://www.youtube.com/embed/hsJidtpHHyo?si=vcA_JU5KcjeQQ4_1&end=147" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe> <br><br>
                        <h4 class="pt-4">Summary</h4>
                        <ul>
                            <li class="pt-1">In a Machine Learning context, <b><i>Features</i></b> are the variables (x) that you're using to predict the target variable (y).
                            <ul>
                                <li><b>Example:</b> Suppose you're trying to predict the price of a home. You collect data from thousands of listings with the following features: <ul>
                                    <li>price</li>
                                    <li>home_size</li>
                                    <li>num_baths</li>
                                    <li>num_beds</li>
                                    <li>yard_size</li>
                                    <li>location</li>
                                </ul>
                                <li><b><i>Price</i></b> would be your target variable (since that's what you're trying to predict), and <b><i>Home Size, # of Bathrooms, # of Bedrooms, Yard Size, and Location</i></b> are your <u><i>features.</i></u></li></li>
                            </ul></li>
                            <li class="pt-2"><b>NOTE:</b> Not all variables present in a dataset can, or should, be used as features. For instance, if you have an ID field, such as an employee or student ID, this is irrelevant for use in a model since it's a randomly assigned value.</li>
                        </ul>
                        <h2 class="mt-5 px-4 px-sm-3 border-start border-3 border-secondary">What is Feature Engineering?</h2> <br>
                        <iframe width="650" height="400" src="https://www.youtube.com/embed/yQ5wTC4E5us?si=-iwtYqId9Cui9uJ0&end=434" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe> <br><br>
                        <h4 class="pt-4">Summary</h4>
                        <ul>
                            <li class="pt-1"><b><i>Feature Engineering</i></b> is the process of cleaning and filtering data AFTER performing EDA to enhance your model's performance. Common tasks in this process include excluding outliers, excluding features that are not statistically significant, or imputing missing values.
                            </li>
                            <li class="pt-2">It's easy to get confused on the difference between Feature Engineering and Data Wrangling. However, just remember that the purpose of <b><i>Data Wrangling</i></b> is to clean and prepare your data for EDA, whereas <b><i>Feature Engineering</i></b> is applying the insights you gained from the EDA process to your dataset to prepare and enhance your dataset for Model Training.
                            </li>
                            <li class="pt-2"><b><i>Obvious Feature Engineering</i></b> involves fixing problems in your training data that can be seen without doing EDA. This can include excluding ID columns, creating calculated fields based on existing features to enhance your model's performance, or adding features from other datasets that are relevant to predicting your target variable.</li>
                            <li class="pt-2"><b><i>Non-Obvious Feature Engineering</i></b> involves fixing problems that you identify in the EDA process. This can include imputing missing values, excluding outliers, or excluding features that aren't correlated with the target variable.</li>
                            <li class="pt-2"><b><i>Domain Knowledge</i></b> is the process of understanding how a specific entity or process works. This typically refers to understanding how your business collects data, creates and uses KPIs to track performance, and general industry knowledge surrounding correlations between variables and knowing what certain acronyms or abbreviations mean. As you get more experience as an ML Engineer, you'll naturally gain domain knowledge either within the business you work for or the industry you're in.</li>
                            <li class="pt-2"><b>NOTE:</b> As mentioned in the video above, Feature Engineering can include encoding categorical values for use in Model Training. However, this step is typically associated with <b><i>Data Preprocessing</i></b>, the next step in the MLOps Lifecycle.</li>
                        </ul>
                        <h2 class="mt-5 px-4 px-sm-3 border-start border-3 border-secondary">Feature Engineering Best Practices</h2> <br>
                        <iframe width="650" height="400" src="https://www.youtube.com/embed/7tW29jBceRw?si=odCX0I6VfFupsZfZ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe> <br><br>
                        <h4 class="pt-4">Summary</h4>
                        <ul>
                            <li class="pt-1"><b><i>Feature Selection</i></b> is the process of selecting a subset of features present in your dataset AFTER performing EDA.
                            </li>
                            <li class="pt-2">Feature Selection is a critical step in the Feature Engineering process and allows you to:
                                <ul>
                                    <li class="pt-1"><b>Reduce the dimensionality of your dataset</b>. If your dataset has more than 100 features, there are likely several features you can trim to make your dataset more readable.</li>
                                    <li class="pt-1"><b>Improve Training Efficiency</b>. When perfoming Hyperparameter Tuning (See Next Tutorial), it helps to have a smaller and more impactful subset of features to more efficiently tune your model's performance.</li>
                                    <li class="pt-1"><b>Remove Noise and Redundancies.</b> If you're using a Gradient-Boosted Model, as discussed in our "Deep Learning" Tutorial Series (Coming Soon), irrelevant features will distract the model from performing calculations on impactful features that will ultimately lead to better results. <b><i>Noise</i></b> is what's created by these irrelevant features and distracts the model from optimizing itself on impactful features.</li>
                                </ul>
                            </li>
                            <li class="pt-2"><b><i>Intrinsic Methods</i></b> use ensembles (random pairings of features) to find and optimize relationships between variables, which ultimately lead to a predictive outcome. Intrinsic Methods are used with the following models: <ul>
                                <li class="pt-1"><b>Tree-Based Models.</b> A common example of this is a <b><i>Random Forest</i></b> which is used with Decision Trees to randomly pair small subsets of features to find the optimal route to the prediction while optimizing your evaluation metric(s).</li>
                                <li class="pt-1"><b>Regularization Models.</b> Models such as Linear Regression, Support Vector Machine (SVMs), and Neural Networks use various regularization methods to assign weights to features based on their impact on the predictive outcome. The higher the weight, the higher the impact. These are very good at reducing noise.</li>
                            </ul></li>
                            <li class="pt-2">Intrinsic Methods Pros and Cons:
                                <ul>
                                    <li class="pt-1"><b>Pros:</b>
                                    <ul>
                                        <li class="pt-1">Efficient, due to the fact that they are embedded in the algorithms.</li>
                                        <li class="pt-1">No external tools or methods need to be used, leading to lower overhead costs.</li>
                                        <li class="pt-1">Optimizes feature importance, leading to optimal model performance.</li>
                                    </ul></li>
                                </ul>
                                <ul>
                                    <li class="pt-1"><b>Cons:</b>
                                    <ul>
                                        <li class="pt-1">Limited number of algorithms have these methods embedded. However, the algorithms that do utilize these methods are commonly used.</li>
                                    </ul></li>
                                </ul>
                            </li>
                            <li class="pt-2"><b><i>Filter Methods</i></b> take advantage of descriptive statistics to visualize the relationships between the target variable and all the features. This approach can be universally applied, whereas Intrinsic Methods are naturally embedded in a limited number of algorithms. These methods can be applied in the following ways:
                            <ul>
                                <li class="pt-1"><b>Univariate Statistical Analysis</b>. This method is primarily used in the EDA process. With this method, you utilize inferential statistics to analyze the relationship between the target variable and another feature. Then, you set a benchmark score (such as a Correlation Score of at least 0.45) to filter out features until you have only the most relevant ones remaining.</li>
                                <li class="pt-1"><b>Feature-Importance Based</b>. This method is used AFTER you fitted your training data to your model with all features present. You then visualize the impact each feature had on predicitng the outcome. You use a benchmark score (such as the one provided by SciKit-Learn) to filter out features that aren't impactful beyond a certain measure. This is commonly used when comparing p-values with Linear and Logistic Regression.</li>
                            </ul></li>
                            <li class="pt-2">Filter Methods Pros and Cons:
                                <ul>
                                    <li class="pt-1"><b>Pros:</b>
                                    <ul>
                                        <li class="pt-1">Simple and fast to compute.</li>
                                        <li class="pt-1">Can be used regardless of algorithm being used.</li>
                                    </ul>
                                </li>
                                </ul>
                                <ul>
                                    <li class="pt-1"><b>Cons:</b>
                                    <ul>
                                        <li class="pt-1">Selection bias present. Commonly associated with selecting redundant or duplicate features.</li>
                                        <li class="pt-1">Ignores relationships between features. Focuses only on relationships between features and the target variable.</li>
                                    </ul></li>
                                </ul>
                            </li>
                            <li class="pt-2"><b><i>Wrapper Methods</i></b> take an iterative approach to feature selection. Rather than being embedded in an algorithm natively, these take random subsets of features, fit those subsets to the model, and output a result. The process ends when the model retrieves the best score possible (score varies based on algorithm being used). Wrapper Methods can be applied in the following ways:
                            <ul>
                                <li class="pt-1"><b>Sequential Feature Selection (SFS)</b>. Tools such as SciKit-Learn have built-in capabilities that alow this process to occur in your ML pipeline. These use a <b><i>greedy search algorithm</i></b> to quickly take every possible combination of features, run it through the algorithm, and output an evaluation score. Once the iteration ends, it select the subset of features that produced the best result and uses those for the evaluation phase. There are two flavors of SFS:
                                <ul>
                                    <li class="pt-1"><b><i>Forward SFS</i></b> starts with the feature that has the highest impact on the target variable, adds another feature, and then cross-validates the score until it retrieves a higher score. It loops through this process until it retrieves the best possible combination of features.</li>
                                    <li class="pt-1"><b><i>Backward SFS</i></b> starts with ALL features present in the dataset and iteratively removes features one by one, starting with the feature with the least impact on the target variable, until it retrieves the best possible combination of features.</li>
                                </ul></li>
                            </ul></li>
                            <li class="pt-2">Wrapper Methods Pros and Cons:
                                <ul>
                                    <li class="pt-1"><b>Pros:</b>
                                    <ul>
                                        <li class="pt-1">Extremely efficient for finding the optimal combination of features.</li>
                                        <li class="pt-1">Considers all features available, rather than a subset.</li>
                                    </ul></li>
                                    <li class="pt-1"><b>Cons:</b>
                                    <ul>
                                        <li class="pt-1">Prone to overfitting features.</li>
                                        <li class="pt-1">Computationally intensive, especially when a large number of features is present.</li>
                                    </ul></li>
                                </ul>
                            </li>
                            <li class="pt-2"><b>NOTE:</b> Domain Knowledge is critical in the Feature Selection process. Since you'll be working with samples of populations, you'll need to use inferential statistics to measure the actual impact a feature will have on the target variable. Domain Knowledge enables you to use a combination of intuition and data-driven insights to select the best features for your model.</li>
                        </ul><br>
                        <!-- Previous/Next Topic buttons. -->
                    <div class="row mt-4 mt-md-5 mb-2 px-3">
                        <div class="col-0 col-sm-8">
                            <a
                            class="col-12 col-sm-4 btn btn-outline-primary py-2 fs-5"
                            href="../Machine Learning/Exploratory-Data-Analysis.html">
                            Previous Topic
                            </a>
                        </div>
                        <a
                            class="col-12 col-sm-4 btn btn-outline-primary py-2 fs-5"
                            href="../Machine Learning/Model-Drifting.html">
                            Next Topic
                        </a>
                    </div>
                </section>
            </div>
            <div class="col-md-1 col-lg-2 col-xxl-3"></div>
        </div>
    </div>
    <!-- Footer at bottom of page. -->
    <footer class="container-fluid bg-secondary">
        <div class="row align-items-center h-100">
            <h4 class="col m-0 text-center text-white">
                © 2023 Data Science Club @ UVU
            </h4>
        </div>
    </footer>
</body>
</html>
